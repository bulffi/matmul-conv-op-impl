#define NRAM_SIZE 512000
#define NRAM_ELEMENT_COUNT (NRAM_SIZE / sizeof(half))

__mlu_entry__ void mlu_matmul_kernel(half* input1, half* input2, half* output, int32_t H, int32_t K, int32_t W) {
  
  // sigle core processing
  // assume any matrix is less than 512KB
  if (taskId > 0) return;
  __nram__ half zeros[NRAM_ELEMENT_COUNT];
  __nramset(zeros, NRAM_ELEMENT_COUNT, 0);
  __memcpy(output, zeros, sizeof(half) * H * W, NRAM2GDRAM);

  __mlu_shared__ half A_buf[NRAM_ELEMENT_COUNT];
  __mlu_shared__ half B_buf[NRAM_ELEMENT_COUNT];

  __memcpy(A_buf, input1, sizeof(half) * H * K, GDRAM2SRAM);
  __memcpy(B_buf, input2, sizeof(half) * K * W, GDRAM2SRAM);

  __bang_printf("%d\n", sizeof(half) * H * K);
  __bang_printf("%hf %hf %hf\n", input1[0], input1[1], A_buf[1]);

  for (int i = 0; i < H; i++) {
    // load a row of A
    half* row = zeros;
    __memcpy(row, A_buf + i * K, sizeof(half) * K, SRAM2NRAM);
    for (int j = 0; j < W; j++) {
      half tempt_sum = 0;
      // load a col of B
      half* col = zeros + (NRAM_ELEMENT_COUNT / 2);
      __memcpy(col, B_buf + j, sizeof(half), SRAM2NRAM, sizeof(half), sizeof(half) * W, K - 1);
      // __bang_mul(col, row, K);
      for (int k = 0; k < K; k++) {
        tempt_sum += row[i] * col[i];
      }
      // __bang_printf("%d %d %d\n", H, K, W);
      // __bang_printf("%hf %hf %hf\n", A_buf[0], A_buf[1], B_buf[0]);
      output[i * W + j] = tempt_sum;
    }
  }

  return;
}